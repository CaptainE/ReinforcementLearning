{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>Work in Progress</font></h4>\n",
    "\n",
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are solving the discrete Lunar Landing v2 environment from OpenAI Gym using 1) REINFORCE and 2) Deep Q-learning and compare the two approaches.\n",
    "\n",
    "**Environment description:**<br>\n",
    "Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import pickle\n",
    "import gym\n",
    "import os\n",
    "from gym import wrappers\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA has been enabled.\n",
      "\n",
      "Running the BreakoutDeterministic-v4 environment...\n",
      "State represented by 210-dimensional vector and 4 actions available.\n"
     ]
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "print('CUDA has been enabled.' if CUDA is True else 'CUDA has been disabled.')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if CUDA else torch.FloatTensor\n",
    "IntTensor   = torch.cuda.IntTensor if CUDA else torch.IntTensor\n",
    "LongTensor  = torch.cuda.LongTensor if CUDA else torch.LongTensor\n",
    "ByteTensor  = torch.cuda.ByteTensor if CUDA else torch.ByteTensor\n",
    "Tensor      = FloatTensor\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "ENV = 'BreakoutDeterministic-v4'\n",
    "print(f'\\nRunning the {ENV} environment...')\n",
    "\n",
    "env = gym.make(ENV)\n",
    "N_OBS_SPACE = env.observation_space.shape[0]\n",
    "N_ACT_SPACE = env.action_space.n\n",
    "print(f'State represented by {N_OBS_SPACE}-dimensional vector and {N_ACT_SPACE} actions available.')\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agent(agent, filename):\n",
    "    '''\n",
    "    Saves an agent of specified filename to relative path.\n",
    "    '''\n",
    "    path = os.getcwd() + '/agents'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    with open(path + '/' + filename + '.agent', 'wb') as f:\n",
    "        pickle.dump(agent, f)\n",
    "    save_stats(agent.test_n, agent.test_r, filename)\n",
    "    \n",
    "\n",
    "def load_agent(filename):\n",
    "    '''\n",
    "    Loads an agent of specified filename from relative path.\n",
    "    '''\n",
    "    with open(os.getcwd() + '/agents' + '/' + filename + '.agent', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "    \n",
    "def save_stats(n, r, filename):\n",
    "    '''\n",
    "    Saves stats of specified filename to relative path.\n",
    "    '''\n",
    "    path = os.getcwd() + '/agents'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    with open(path + '/' + filename + '.stats', 'wb') as f:\n",
    "        pickle.dump((n, r), f)\n",
    "\n",
    "        \n",
    "def load_stats(filename):\n",
    "    '''\n",
    "    Loads stats of specified filename from relative path.\n",
    "    '''\n",
    "    with open(os.getcwd() + '/agents' + '/' + filename + '.stats', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    \n",
    "class Agent(object):\n",
    "    def __init__(self,\n",
    "                 policy=None,\n",
    "                 env=gym.make(ENV),\n",
    "                 num_episodes=1000,\n",
    "                 discount_factor=0.99,\n",
    "                 lr=1e-3,\n",
    "                 test_freq=200,\n",
    "                 test_num=10,\n",
    "                 min_reward=-250,\n",
    "                 max_reward=200):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        self.env = env\n",
    "        self.num_episodes = num_episodes\n",
    "        self.discount_factor = discount_factor\n",
    "        self.lr = lr\n",
    "        self.test_freq = test_freq\n",
    "        self.test_num = test_num\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.achieved_max_reward = False\n",
    "        #self.rollout_limit = env.spec.timestep_limit\n",
    "        self.rollout_limit = 10000\n",
    "        \n",
    "        \n",
    "        if policy is not None:\n",
    "            self.policy = policy.cuda() if CUDA else policy\n",
    "            self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.train_n, self.train_r = [], []\n",
    "        self.test_n, self.test_r = [], []\n",
    "        self.losses = []\n",
    "           \n",
    "    \n",
    "    def reset_env(self, env=None):\n",
    "        \"\"\"\n",
    "        Resets the current environment using a constant\n",
    "        seed to make sure environment is deterministic.\n",
    "        \"\"\"\n",
    "        if env is None: env = self.env\n",
    "        env.seed(0)\n",
    "        return env.reset()\n",
    "    \n",
    "    \n",
    "    def select_action(self, s):\n",
    "        \"\"\"\n",
    "        Selects an action according to the current policy.\n",
    "        \"\"\"\n",
    "        s = Variable(Tensor(s))\n",
    "        action_logits = self.policy(s) \n",
    "        log_probs = action_logits-torch.logsumexp(action_logits, dim = 1)\n",
    "\n",
    "        action = torch.distributions.Categorical(logits=action_logits).sample()\n",
    "        \n",
    "        return action.data.cpu().numpy(), log_probs[0,action.data.cpu().numpy()]\n",
    "    \n",
    "    \n",
    "    def preprocess(self, x):\n",
    "        x = torch.tensor(x).permute([2, 0, 1]).data.numpy()\n",
    "        x = np.mean(x[:, ::2, ::2], axis=0) / 255\n",
    "        return x.reshape(-1, 1, 105, 80)\n",
    "    \n",
    "    def play_episode(self, env=None, replay=False):\n",
    "        \"\"\"\n",
    "        Plays a single episode and returns SAR rollout.\n",
    "        The logarithm of the action probabilities is also\n",
    "        included in the rollout (for computing the loss).\n",
    "        \"\"\"\n",
    "        if env is None: env = self.env\n",
    "        s = self.reset_env(env)\n",
    "        rollout, eps_r, mapo_traj = [], 0, []\n",
    "        s = self.preprocess(s)\n",
    "        for i in range(self.rollout_limit):\n",
    "            a, log_probs = self.select_action(s)\n",
    "            s1, r, done, _ = env.step(a)\n",
    "            s1 = self.preprocess(s1)\n",
    "            rollout.append((s, a, r, log_probs))\n",
    "            if done: r += -1\n",
    "            eps_r += r\n",
    "            if hasattr(self, 'memory'):\n",
    "                self.memory.push(Tensor(s), a, Tensor(s1), r)\n",
    "            if replay: self.replay()\n",
    "            if eps_r < self.min_reward and env is None: break\n",
    "            if done: break\n",
    "            s = s1\n",
    "\n",
    "        if eps_r > self.max_reward:\n",
    "            #print('Achieved maximum reward:', eps_r)\n",
    "            self.achieved_max_reward = True\n",
    "            \n",
    "        return np.array(rollout)\n",
    "    \n",
    "    \n",
    "    def compute_loss(self, rewards, log_probs):\n",
    "        \"\"\"\n",
    "        Computes the loss from discounted return.\n",
    "        \"\"\"\n",
    "        \n",
    "        G, loss = torch.zeros(1,1).type(FloatTensor), 0\n",
    "        rewards = (rewards-np.mean(rewards))/(np.std(rewards)+1e-05)\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            G = self.discount_factor * G + (rewards[i])\n",
    "            loss = loss - (log_probs[i]*Variable(G))\n",
    "            #loss = loss - ((log_probs[i]/log_probs[i])*Variable(G))\n",
    "            \n",
    "        return loss #/ len(rewards)\n",
    "    \"\"\"\n",
    "        G, loss = torch.zeros(1,1).type(FloatTensor), 0\n",
    "\n",
    "        rewards = (rewards-np.mean(rewards))/np.std(rewards)\n",
    "        \n",
    "        for i in reversed(range(len(rewards))):\n",
    "            G = self.discount_factor * G + (rewards[i])\n",
    "            state_val = Variable(G)\n",
    "            loss = loss - state_val\n",
    "        \n",
    "        loss = Variable(loss.data, requires_grad=True)\n",
    "        return loss \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Runs a full training for defined number of episodes.\n",
    "        \"\"\"\n",
    "        last100 = np.zeros(10)\n",
    "        for e in range(1, self.num_episodes+1):\n",
    "            rollout = self.play_episode()\n",
    "            self.optimize(rollout)\n",
    "\n",
    "            if e % self.test_freq == 0:\n",
    "                n, r = self.test()\n",
    "                print('{:5d},  Reward: {:6.2f},  Length: {:4.2f}'.format(e, r, n))\n",
    "                last100[(e//10)%10] = r\n",
    "            if np.mean(last100) >= 200: break\n",
    "        print('Completed training!')\n",
    "        self.plot_rewards()\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Runs a number of tests and computes the\n",
    "        mean episode length and mean reward.\n",
    "        \"\"\"\n",
    "        n, r = [], []\n",
    "\n",
    "        for e in range(self.test_num):\n",
    "            rollout = self.play_episode()     \n",
    "            rewards = np.array(rollout[:,2], dtype=float)\n",
    "            n.append(len(rollout))\n",
    "            r.append(sum(rewards))\n",
    "\n",
    "        #self.test_n.append(n)\n",
    "        #self.test_r.append(r)\n",
    "        #save_agent(self, 'ConvDQN-solved_' + str(len(self.test_n)))\n",
    "        \n",
    "        return np.mean(n), np.mean(r)\n",
    "    \n",
    "    \n",
    "    def get_replay(self):\n",
    "        \"\"\"\n",
    "        Renders an episode replay using the current policy.\n",
    "        \"\"\"\n",
    "        env = wrappers.Monitor(self.env, \"./gym-results\", force=True)\n",
    "        if hasattr(self, 'epsilon'):\n",
    "            eps = self.epsilon\n",
    "            self.epsilon = 0\n",
    "            self.play_episode(env)\n",
    "            self.epsilon = eps\n",
    "        else:\n",
    "            self.play_episode(env)\n",
    "        \n",
    "        \n",
    "    def plot_rewards(self):\n",
    "        \"\"\"\n",
    "        Plots the moving average of the reward during training.\n",
    "        \"\"\"\n",
    "        def moving_average(a, n=10) :\n",
    "            ret = np.cumsum(a, dtype=float)\n",
    "            ret[n:] = ret[n:] - ret[:-n]\n",
    "            return ret / n\n",
    "        \n",
    "        plt.figure(figsize=(16,6))\n",
    "        plt.subplot(211)\n",
    "        plt.plot(range(1, len(self.train_r)+1), self.train_r, label='training reward')\n",
    "        plt.plot(moving_average(self.train_r))\n",
    "        plt.xlabel('episode'); plt.ylabel('reward')\n",
    "        plt.xlim((0, len(self.train_r)))\n",
    "        plt.legend(loc=4); plt.grid()\n",
    "        plt.subplot(212)\n",
    "        plt.plot(range(1, len(self.losses)+1), self.losses, label='loss')\n",
    "        plt.plot(moving_average(self.losses))\n",
    "        plt.xlabel('episode'); plt.ylabel('loss')\n",
    "        plt.xlim((0, len(self.losses)))\n",
    "        plt.legend(loc=4); plt.grid()\n",
    "        plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "   50,  Reward:   5.00,  Length: 266.70\n",
      "  100,  Reward:   4.00,  Length: 268.70\n",
      "  150,  Reward:   5.60,  Length: 283.30\n",
      "  200,  Reward:   7.80,  Length: 340.40\n"
     ]
    }
   ],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_hidden1=32,\n",
    "                 n_hidden2=32):        \n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_OBS_SPACE, n_hidden1)\n",
    "        self.fc2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        #self.fc3 = nn.Linear(n_hidden2, n_hidden3)\n",
    "        self.out = nn.Linear(n_hidden2, N_ACT_SPACE)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return F.softmax(self.out(x), dim=0)\n",
    "\n",
    "class Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, padding=False):\n",
    "        super(Conv, self).__init__()\n",
    "\n",
    "        padding = int((kernel_size - 1) / 2) if padding else 0\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, dilation=dilation, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            #nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class PolicyConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_out):\n",
    "        super(PolicyConvNet, self).__init__()\n",
    "\n",
    "        self.conv1 = Conv(1, 16, kernel_size=8, stride=4)\n",
    "        self.conv2 = Conv(16, 32, kernel_size=4, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(2816, 128)\n",
    "        self.out = nn.Linear(128, n_out)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 1, 105, 80)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = x.reshape(-1, 2816)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.out(x)\n",
    "    \n",
    "    \n",
    "class REINFORCE(Agent):\n",
    "    def __init__(self,\n",
    "                 env=gym.make(ENV),\n",
    "                 num_episodes=10000,\n",
    "                 discount_factor=0.99,\n",
    "                 lr=1e-3,\n",
    "                 test_freq=250,\n",
    "                 test_num=10):\n",
    "        super(REINFORCE, self).__init__(\n",
    "            policy=PolicyConvNet(n_out = N_ACT_SPACE),\n",
    "            env=env,\n",
    "            num_episodes=num_episodes,\n",
    "            discount_factor=discount_factor,\n",
    "            lr=lr,\n",
    "            test_freq=test_freq,\n",
    "            test_num=test_num)\n",
    "\n",
    "\n",
    "    def optimize(self, rollout):\n",
    "        rewards = np.array(rollout[:,2], dtype=float)\n",
    "        log_probs = np.array(rollout[:,3])\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.compute_loss(rewards, log_probs)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        #self.scheduler.step()\n",
    "                   \n",
    "        self.train_r.append(sum(rewards))\n",
    "        self.train_n.append(len(rollout)) \n",
    "        self.losses.append(loss.detach().cpu())\n",
    "        \n",
    "        \n",
    "agent = REINFORCE(num_episodes=2500,lr = 1e-4, test_freq = 50)\n",
    "print('start training')\n",
    "agent.train()\n",
    "agent.get_replay()\n",
    "save_agent(agent, 'ConvREINFORCE-solved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get_replay()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size = 3, dilation = 1, padding = False):\n",
    "        super(Conv, self).__init__()\n",
    "        \n",
    "        padding = int((kernel_size-1)/2) if padding else 0\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, dilation=dilation, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class PolicyConvNet(nn.Module):\n",
    "    def __init__(self):        \n",
    "        super(PolicyConvNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv(3, 32)\n",
    "        self.conv2 = Conv(32, 24)\n",
    "        self.conv3 = Conv(24, 16)\n",
    "        \n",
    "        self.fc1 = nn.Linear(6912, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 128)\n",
    "        self.out = nn.Linear(128, N_ACT_SPACE)\n",
    "        \n",
    "\n",
    "    def forward(self, x):  \n",
    "        x = x.reshape(-1, 3, 210, 160)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.reshape(-1, 6912)   \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_hidden1=64,\n",
    "                 n_hidden2=64):        \n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(N_OBS_SPACE, n_hidden1)\n",
    "        self.fc2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.out = nn.Linear(n_hidden2, N_ACT_SPACE)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class DQN(Agent):\n",
    "    def __init__(self,\n",
    "                 env=gym.make(ENV),\n",
    "                 num_episodes=10000,\n",
    "                 discount_factor=0.99,\n",
    "                 lr=1e-4,\n",
    "                 test_freq=500,\n",
    "                 test_num=10,\n",
    "                 update_target=10,\n",
    "                 epsilon=1.0,\n",
    "                 epsilon_decay=0.99993,\n",
    "                 epsilon_min=0.01,\n",
    "                 mapo=True):\n",
    "        super(DQN, self).__init__(\n",
    "            policy=PolicyConvNet(),\n",
    "            env=env,\n",
    "            num_episodes=num_episodes,\n",
    "            discount_factor=discount_factor,\n",
    "            lr=lr,\n",
    "            test_freq=test_freq,\n",
    "            test_num=test_num)\n",
    "        \n",
    "        self.target = PolicyConvNet()\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "        self.target.eval()\n",
    "        \n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        if CUDA:\n",
    "            self.target = self.target.cuda()\n",
    "        \n",
    "        self.update_target = update_target\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        \n",
    "        \n",
    "    def select_action(self, s):\n",
    "        \"\"\"\n",
    "        Selects an action according to the current policy.\n",
    "        \"\"\"\n",
    "        if random.random() > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                return int(self.policy(Tensor(s)).argmax().cpu().data.numpy()), None\n",
    "        else:\n",
    "            return np.random.randint(N_ACT_SPACE), None\n",
    "        \n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), dtype=torch.uint8)\n",
    "        non_final_next_states = torch.cat([Tensor(s) for s in batch.next_state\n",
    "                                                    if s is not None]).reshape(BATCH_SIZE, -1)\n",
    "\n",
    "        state_batch = torch.cat(batch.state).reshape(BATCH_SIZE, -1)\n",
    "        action_batch = torch.cat(torch.split(LongTensor(batch.action), 1)).reshape(BATCH_SIZE, -1)\n",
    "        reward_batch = Tensor(batch.reward)\n",
    "\n",
    "        state_action_values = self.policy(state_batch).gather(1, action_batch)\n",
    "\n",
    "        next_state_values = Tensor(torch.zeros(BATCH_SIZE))\n",
    "        \n",
    "        next_state_values[non_final_mask] = self.target(non_final_next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = ((next_state_values * self.discount_factor) + reward_batch)\n",
    "        \n",
    "        loss = self.loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Runs a full training for defined number of episodes.\n",
    "        \"\"\"\n",
    "        for e in range(1, self.num_episodes+1):\n",
    "            rollout = self.play_episode(replay=True)\n",
    "\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "            if e % self.update_target == 0:\n",
    "                self.target.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "            if e % self.test_freq == 0:\n",
    "                n, r = self.test()\n",
    "                print('{:5d},  Reward: {:6.2f}, , Length: {:4.2f}, Epsilon: {:4.2f}'.format(e, r, n, self.epsilon))\n",
    "                \n",
    "            if self.achieved_max_reward: break\n",
    "\n",
    "        print('Completed training!')\n",
    "        \n",
    "    \n",
    "    def optimize(self, rollout):\n",
    "        rewards = np.array(rollout[:,2], dtype=float)\n",
    "        log_probs = np.array(rollout[:,3])\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.compute_loss(rewards, log_probs)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #self.train_r.append(sum(rewards))\n",
    "        #self.train_n.append(len(rollout))\n",
    "        #self.losses.append(loss.detach().cpu())\n",
    "        \n",
    "        \n",
    "agent = DQN()\n",
    "agent.train()\n",
    "agent.get_replay()\n",
    "save_agent(agent, 'DQN-solved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
