{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><font color='red'>Work in Progress</font></h4>\n",
    "\n",
    "# DQN - Breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import pickle\n",
    "import gym\n",
    "import os\n",
    "from gym import wrappers\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA has been enabled.\n",
      "\n",
      "Running the BreakoutDeterministic-v4 environment...\n",
      "State represented by 210-dimensional vector and 4 actions available.\n"
     ]
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "print('CUDA has been enabled.' if CUDA is True else 'CUDA has been disabled.')\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if CUDA else torch.FloatTensor\n",
    "IntTensor   = torch.cuda.IntTensor if CUDA else torch.IntTensor\n",
    "LongTensor  = torch.cuda.LongTensor if CUDA else torch.LongTensor\n",
    "ByteTensor  = torch.cuda.ByteTensor if CUDA else torch.ByteTensor\n",
    "Tensor      = FloatTensor\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "ENV = 'BreakoutDeterministic-v4'\n",
    "print(f'\\nRunning the {ENV} environment...')\n",
    "\n",
    "env = gym.make(ENV)\n",
    "N_OBS_SPACE = env.observation_space.shape[0]\n",
    "N_ACT_SPACE = env.action_space.n\n",
    "print(f'State represented by {N_OBS_SPACE}-dimensional vector and {N_ACT_SPACE} actions available.')\n",
    "#del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_agent(agent, filename):\n",
    "    '''\n",
    "    Saves an agent of specified filename to relative path.\n",
    "    '''\n",
    "    path = os.getcwd() + '/agents'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    with open(path + '/' + filename + '.agent', 'wb') as f:\n",
    "        pickle.dump(agent, f)\n",
    "    save_stats(agent.test_n, agent.test_r, filename)\n",
    "    \n",
    "\n",
    "def load_agent(filename):\n",
    "    '''\n",
    "    Loads an agent of specified filename from relative path.\n",
    "    '''\n",
    "    with open(os.getcwd() + '/agents' + '/' + filename + '.agent', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "    \n",
    "def save_stats(n, r, filename):\n",
    "    '''\n",
    "    Saves stats of specified filename to relative path.\n",
    "    '''\n",
    "    path = os.getcwd() + '/agents'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    with open(path + '/' + filename + '.stats', 'wb') as f:\n",
    "        pickle.dump((n, r), f)\n",
    "\n",
    "        \n",
    "def load_stats(filename):\n",
    "    '''\n",
    "    Loads stats of specified filename from relative path.\n",
    "    '''\n",
    "    with open(os.getcwd() + '/agents' + '/' + filename + '.stats', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "    \n",
    "def load_memory(filename='human'):\n",
    "    '''\n",
    "    Loads human play memory from relative path.\n",
    "    '''\n",
    "    with open(os.getcwd() + '/memory/' + filename + '.memory', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    \n",
    "class Agent(object):\n",
    "    def __init__(self,\n",
    "                 policy=None,\n",
    "                 env=gym.make(ENV),\n",
    "                 num_episodes=1000,\n",
    "                 discount_factor=0.99,\n",
    "                 lr=1e-3,\n",
    "                 test_freq=200,\n",
    "                 test_num=10,\n",
    "                 min_reward=-250,\n",
    "                 max_reward=200):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        self.env = env\n",
    "        self.num_episodes = num_episodes\n",
    "        self.discount_factor = discount_factor\n",
    "        self.lr = lr\n",
    "        self.test_freq = test_freq\n",
    "        self.test_num = test_num\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.achieved_max_reward = False\n",
    "        self.rollout_limit = env.spec.timestep_limit\n",
    "        \n",
    "        if policy is not None:\n",
    "            self.policy = policy.cuda() if CUDA else policy\n",
    "            self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.test_n, self.test_r = [], []\n",
    "        self.losses = []\n",
    "           \n",
    "    \n",
    "    def reset_env(self, env=None):\n",
    "        \"\"\"\n",
    "        Resets the current environment using a constant\n",
    "        seed to make sure environment is deterministic.\n",
    "        \"\"\"\n",
    "        if env is None: env = self.env\n",
    "        env.seed(0)\n",
    "        return self.preprocess(env.reset())\n",
    "    \n",
    "    \n",
    "    def select_action(self, s):\n",
    "        \"\"\"\n",
    "        Selects an action according to the current policy.\n",
    "        \"\"\"\n",
    "        s = Variable(Tensor(s))\n",
    "        action_probs = self.policy(s)\n",
    "        log_probs = action_probs.log()\n",
    "        action = torch.distributions.Categorical(action_probs).sample()\n",
    "        return action.data.cpu().numpy(), log_probs[action]\n",
    "    \n",
    "    \n",
    "    def take_action(self,state):\n",
    "        preprocessedstate = self.preprocess(state)\n",
    "        action = self.select_action(preprocessedstate)\n",
    "        return action[0]\n",
    "    \n",
    "    \n",
    "    def preprocess(self, x):\n",
    "        x = torch.tensor(x).permute([2, 0, 1]).data.numpy()\n",
    "        x = np.mean(x[:, ::2, ::2], axis=0) / 255\n",
    "        return x.reshape(-1, 1, 105, 80)\n",
    "    \n",
    "    \n",
    "    def play_episode(self, env=None, replay=False):\n",
    "        \"\"\"\n",
    "        Plays a single episode and returns SAR rollout.\n",
    "        The logarithm of the action probabilities is also\n",
    "        included in the rollout (for computing the loss).\n",
    "        \"\"\"\n",
    "        train = env is None\n",
    "        if train:\n",
    "            env = self.env\n",
    "            \n",
    "        s = self.reset_env(env)\n",
    "        rollout, eps_r = [], 0\n",
    "\n",
    "        for i in range(self.rollout_limit):\n",
    "            a, log_probs = self.select_action(s)\n",
    "            s1, r, done, _ = env.step(a)\n",
    "            s1 = self.preprocess(s1)          \n",
    "            eps_r += r\n",
    "            \n",
    "            if done:\n",
    "                r = -1\n",
    "                \n",
    "            rollout.append((s, a, r, log_probs))\n",
    "            \n",
    "            if train:            \n",
    "                if self.epsilon > self.epsilon_min:\n",
    "                    self.epsilon -= (self.epsilon_max - self.epsilon_min) / self.epsilon_steps\n",
    "                    \n",
    "                if hasattr(self, 'memory'):\n",
    "                    self.memory.push(Tensor(s), a, Tensor(s1), r)\n",
    "                    self.step_num += 1\n",
    "                    if self.step_num % self.update_target == 0:\n",
    "                        self.step_num = 0\n",
    "                        self.target.load_state_dict(self.policy.state_dict())\n",
    "                        if CUDA:\n",
    "                            self.target = self.target.cuda()\n",
    "                        \n",
    "                if CUDA:\n",
    "                    self.target = self.target.cuda()\n",
    "                    \n",
    "                if replay: self.replay()\n",
    "                if eps_r < self.min_reward and env is None: break\n",
    "                    \n",
    "            if done: break\n",
    "                     \n",
    "            s = s1\n",
    "\n",
    "        if eps_r > self.max_reward:\n",
    "            print('Achieved maximum reward:', eps_r)\n",
    "            self.achieved_max_reward = True\n",
    "            \n",
    "        return np.array(rollout)\n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Runs a number of tests and computes the\n",
    "        mean episode length and mean reward.\n",
    "        \"\"\"\n",
    "        n, r = [], []\n",
    "\n",
    "        for e in range(self.test_num):\n",
    "            rollout = self.play_episode()     \n",
    "            rewards = np.array(rollout[:,2], dtype=float)\n",
    "            n.append(len(rollout))\n",
    "            r.append(sum(rewards))\n",
    "\n",
    "        self.test_n.append(n)\n",
    "        self.test_r.append(r)\n",
    "        return np.mean(n), np.mean(r)\n",
    " \n",
    "\n",
    "    def get_replay(self):\n",
    "        \"\"\"\n",
    "        Renders an episode replay using the current policy.\n",
    "        \"\"\"\n",
    "        env = wrappers.Monitor(self.env, \"./gym-results\", force=True)\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            env.render()\n",
    "            action = agent.take_action(state)\n",
    "            state_next, reward, terminal, info = env.step(action)\n",
    "            state = state_next\n",
    "            if terminal: break\n",
    "\n",
    "        env.close()\n",
    "            \n",
    "    \n",
    "    def render(self):\n",
    "        if hasattr(self, 'epsilon'):\n",
    "            eps = self.epsilon\n",
    "            self.epsilon = 0\n",
    "            self.play_episode(self.env, render=True)\n",
    "            self.epsilon = eps\n",
    "        else:\n",
    "            self.play_episode(self.env, render=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling memory...\n",
      "Started training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d5b7271e06a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#load_agent('ConvDQN-solved') #DQN()\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;31m#agent.pretrain()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[0msave_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ConvDQN-solved'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-d5b7271e06a9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m             \u001b[0mrollout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_freq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-a17e8bf4cf82>\u001b[0m in \u001b[0;36mplay_episode\u001b[1;34m(self, env, replay)\u001b[0m\n\u001b[0;32m    124\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mreplay\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0meps_r\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_reward\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0menv\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-d5b7271e06a9>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-d5b7271e06a9>\u001b[0m in \u001b[0;36mupdate_weights\u001b[1;34m(self, transitions)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m             \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Conv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, padding=False):\n",
    "        super(Conv, self).__init__()\n",
    "\n",
    "        padding = int((kernel_size - 1) / 2) if padding else 0\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, dilation=dilation, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class PolicyConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_out):\n",
    "        super(PolicyConvNet, self).__init__()\n",
    "\n",
    "        self.conv1 = Conv(1, 16, kernel_size=8, stride=4)\n",
    "        self.conv2 = Conv(16, 32, kernel_size=4, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(2816, 128)\n",
    "        self.out = nn.Linear(128, n_out)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 1, 105, 80)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = x.reshape(-1, 2816)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class DQN(Agent):\n",
    "    def __init__(self,\n",
    "                 env_arg='BreakoutDeterministic-v4',\n",
    "                 env=gym.make('BreakoutDeterministic-v4'),\n",
    "                 num_episodes=1000,\n",
    "                 discount_factor=0.99,\n",
    "                 lr=3e-4,\n",
    "                 test_freq=50,\n",
    "                 test_num=10,\n",
    "                 update_target=2500,\n",
    "                 epsilon_max=1,\n",
    "                 epsilon_min=0.1,\n",
    "                 epsilon_steps=1000000//4,\n",
    "                 name=None):\n",
    "        super(DQN, self).__init__(\n",
    "            env=gym.make(env_arg),\n",
    "            policy=PolicyConvNet(n_out=env.action_space.n),\n",
    "            num_episodes=num_episodes,\n",
    "            discount_factor=discount_factor,\n",
    "            lr=lr,\n",
    "            test_freq=test_freq,\n",
    "            test_num=test_num)\n",
    "        \n",
    "        self.inputsize = env.observation_space.shape[0]\n",
    "        self.outputsize = env.action_space.n\n",
    "\n",
    "        if CUDA:\n",
    "            self.policy = self.policy.cuda()\n",
    "            \n",
    "        self.target = PolicyConvNet(n_out=env.action_space.n)\n",
    "        self.target.load_state_dict(self.policy.state_dict())\n",
    "        self.target.eval()\n",
    "        \n",
    "        if CUDA:\n",
    "            self.target = self.target.cuda()\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.memory = ReplayMemory(100000)\n",
    "        self.update_target = update_target\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.epsilon = epsilon_max\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_steps = epsilon_steps\n",
    "        self.step_num = 0\n",
    "        \n",
    "        \n",
    "    def select_action(self, s):\n",
    "        \"\"\"\n",
    "        Selects an action according to the current policy.\n",
    "        \"\"\"\n",
    "        if random.random() > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                return int(self.policy(Tensor(s)).argmax().cpu().data.numpy()), None\n",
    "        else:\n",
    "            return np.random.randint(self.outputsize), None\n",
    "    \n",
    "    \n",
    "    def update_weights(self, transitions):\n",
    "        \"\"\"\n",
    "        Updates the network's weights according to a\n",
    "        number of given transitions.\n",
    "        \"\"\"\n",
    "        minibatch_size = len(transitions)\n",
    "        \n",
    "        batch = Transition(*zip(*transitions))\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), dtype=torch.uint8)\n",
    "\n",
    "        non_final_next_states = torch.cat([Tensor(s) for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        \n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(torch.split(LongTensor(batch.action), 1)).reshape(minibatch_size, -1)\n",
    "        reward_batch = Tensor(batch.reward)\n",
    "\n",
    "        state_action_values = self.policy(state_batch).gather(1, action_batch)\n",
    "\n",
    "        next_state_values = Tensor(torch.zeros(minibatch_size).cuda() if CUDA else torch.zeros(minibatch_size))\n",
    "        \n",
    "        next_state_values[non_final_mask] = self.target(non_final_next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = ((next_state_values * self.discount_factor) + reward_batch)\n",
    "        \n",
    "        loss = self.loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Samples a number of transitions from memory and replays\n",
    "        the experiencies. Does not do anything until memory contains\n",
    "        enough samples for a full minibatch.\n",
    "        \"\"\"\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "        self.update_weights(transitions)\n",
    "    \n",
    "    \n",
    "    def pretrain(self, batches=10000, batch_size=32):\n",
    "        print('Pretraining from memory...')\n",
    "        \n",
    "        self.memory = load_memory()\n",
    "        \n",
    "        for batch in range(1, batches):\n",
    "            transitions = self.memory.sample(batch_size)\n",
    "            self.update_weights(transitions)\n",
    "            \n",
    "            if batch % (batches // 5) == 0:\n",
    "                print(str(int(batch*100//batches)) + '%')\n",
    "                \n",
    "        print('Completed pretraining!\\n')\n",
    "        \n",
    "        \n",
    "    def fill_memory(self, episodes=300):\n",
    "        env = self.env     \n",
    "        s = self.reset_env(env)\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            for i in range(self.rollout_limit):\n",
    "                a = np.random.randint(self.outputsize)\n",
    "                s1, r, done, _ = env.step(a)\n",
    "                s1 = self.preprocess(s1)\n",
    "\n",
    "                if done:\n",
    "                    r = -1\n",
    "\n",
    "                self.memory.push(Tensor(s), a, Tensor(s1), r)    \n",
    "                if done: break\n",
    "                s = s1\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Runs a full training for a defined number of episodes.\n",
    "        \"\"\"\n",
    "        print('Filling memory...')\n",
    "        self.fill_memory()\n",
    "        print('Started training...')\n",
    "        \n",
    "        for e in range(1, self.num_episodes+1):\n",
    "            rollout = self.play_episode(replay=True)\n",
    "            \n",
    "            if e % self.test_freq == 0:\n",
    "                n, r = self.test()\n",
    "                print('{:5d},  Reward: {:6.2f},  Length: {:4.2f}, Epsilon: {:4.2f}'.format(e, r, n, self.epsilon))\n",
    "                \n",
    "            if self.achieved_max_reward: break\n",
    "\n",
    "        print('Completed training!')\n",
    "        \n",
    "\n",
    "agent = DQN() #load_agent('ConvDQN-solved')\n",
    "#agent.pretrain()\n",
    "agent.train()\n",
    "save_agent(agent, 'ConvDQN-solved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.get_replay()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrappers.Monitor(env, \"./gym-results\", force=True)\n",
    "state = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    action = agent.take_action(state)\n",
    "    state_next, reward, terminal, info = env.step(action)\n",
    "    state = state_next\n",
    "    if terminal:\n",
    "        break\n",
    "\n",
    "    # take a random action\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.take_action(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load human memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = load_memory()\n",
    "\n",
    "print('Human memory of length', len(mem.memory))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(mem.memory[0].state.reshape(105, 80).cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
